{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import Audio\n",
    "import scipy.signal as sig\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "import mat73\n",
    "import os\n",
    "from masp import shoebox_room_sim as srs\n",
    "from os.path import join as pjoin\n",
    "import copy\n",
    "import spaudiopy as spa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import my modules (helpers.py where I stored all the functions):\n",
    "import helpers as hlp\n",
    "import importlib\n",
    "importlib.reload(hlp);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_path = 'sounds'\n",
    "arte_path = '/Users/joanna.luberadzka/Data/ARTE'\n",
    "output_path = 'generated-sounds/'\n",
    "maxlim = 2\n",
    "ambi_order = 10\n",
    "fs=48000\n",
    "target_snr = -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mono2biSH(mono_sig, sh_rir):\n",
    "    # Apply audio to SH IR\n",
    "    # sh_rir: (M, maxSH, 2, 1), which comes from (M, maxSH, nRec, nSrc)\n",
    "    # mono_sig: (1,D)\n",
    "    left = sig.fftconvolve(np.tile(mono_sig[0], (121,1)).T, sh_rir[:,:,0,0], 'full', 0)   \n",
    "    right = sig.fftconvolve(np.tile(mono_sig[0], (121,1)).T, sh_rir[:,:,1,0], 'full', 0)\n",
    "    return np.array([left, right])\n",
    "\n",
    "def mono2sh(mono_sig, sh_rir):\n",
    "    # Apply audio to SH IR\n",
    "    # sh_rir: (M, maxSH, 1, 1), which comes from (M, maxSH, nRec, nSrc)\n",
    "    # mono_sig: (1,D)\n",
    "    head = sig.fftconvolve(np.tile(mono_sig[0], (121,1)).T, sh_rir[:,:,0,0], 'full', 0)   \n",
    "    return head\n",
    "\n",
    "def biSH2bin(sh_sig, decoder):\n",
    "    left = sig.fftconvolve(sh_sig[0], decoder[:,:,0], 'full', 0).sum(1)\n",
    "    right = sig.fftconvolve(sh_sig[1], decoder[:,:,1], 'full', 0).sum(1)\n",
    "    return np.array([left,right])\n",
    "\n",
    "def sh2bin(sh_sig, decoder):\n",
    "    left = sig.fftconvolve(sh_sig, decoder[:,:,0], 'full', 0).sum(1)\n",
    "    right = sig.fftconvolve(sh_sig, decoder[:,:,1], 'full', 0).sum(1)\n",
    "    return np.array([left,right])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DESIGN A SCENE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --------------- DESIGN PARTY SCENE ----------------\n",
    "# scene_tag=\"party\"\n",
    "# # --- Room: ----\n",
    "# room = np.array([15., 10., 3.5]) \n",
    "# rt60 = np.array([.4])\n",
    "# # --- Receivers: ----\n",
    "# head_orient = np.array([0, 0])\n",
    "# head_pos= np.array([room[0]/2, room[1]/2, 1.6]) # Listener coordinates\n",
    "# ears_pos=hlp.head_2_ku_ears(head_pos,head_orient)\n",
    "# mics = np.array([ears_pos[0], ears_pos[1], list(head_pos)]) #two positions for the binaural, one for the SH\n",
    "# # --- Sources: ----\n",
    "# TARGET1_ANGLE=20\n",
    "# TARGET2_ANGLE=-20\n",
    "# src1_pos= hlp.place_on_circle(head_pos,1,TARGET1_ANGLE)\n",
    "# src2_pos= hlp.place_on_circle(head_pos,1,TARGET2_ANGLE)\n",
    "# srcs=np.array(src1_pos+src2_pos)\n",
    "# # --- Audio: ----\n",
    "# src1_sig_mono, fs_src1 = sf.read(pjoin(speech_path, 'dial2p1.wav'))\n",
    "# src2_sig_mono, fs_src2 = sf.read(pjoin(speech_path, 'dial2p2.wav'))\n",
    "# noise_sig_sh, fs_noise = sf.read(pjoin(arte_path,'09_Dinner_party_MOA_31ch/09_Dinner_party_MOA_31ch.wav'))\n",
    "# chunk_len = 25 #seconds\n",
    "\n",
    "# --------------- DESIGN RESTAURANT SCENE ----------------\n",
    "scene_tag=\"restaurant\"\n",
    "# --- Room: ----\n",
    "room = np.array([28., 17., 4.2]) \n",
    "rt60 = np.array([1.1]) * 0.5\n",
    "# --- Receivers: ----\n",
    "head_orient = np.array([0, 0])\n",
    "head_pos= np.array([room[0]/2, room[1]/2, 1.3]) # Listener coordinates\n",
    "ears_pos=hlp.head_2_ku_ears(head_pos,head_orient)\n",
    "mics = np.array([ears_pos[0], ears_pos[1], list(head_pos)]) #two positions for the binaural, one for the SH\n",
    "# --- Sources: ----\n",
    "TARGET1_ANGLE=20\n",
    "TARGET2_ANGLE=-20\n",
    "src1_pos= hlp.place_on_circle(head_pos,1,TARGET1_ANGLE)\n",
    "src2_pos= hlp.place_on_circle(head_pos,1,TARGET2_ANGLE)\n",
    "srcs=np.array(src1_pos+src2_pos)\n",
    "# --- Audio: ----\n",
    "src1_sig_mono, fs_src1 = sf.read(pjoin(speech_path, 'dial3p1.wav'))\n",
    "src2_sig_mono, fs_src2 = sf.read(pjoin(speech_path, 'dial3p2.wav'))\n",
    "noise_sig_sh, fs_noise = sf.read(pjoin(arte_path,'08_Cafe_2_MOA_31ch/08_Cafe_2_MOA_31ch.wav'))\n",
    "chunk_len = 25 #seconds\n",
    "\n",
    "\n",
    "# # --------------- DESIGN MEETING SCENE ----------------\n",
    "# scene_tag=\"meeting\"\n",
    "# # --- Room: ----\n",
    "# room = np.array([5., 2., 2.5]) \n",
    "# rt60 = np.array([0.2]) * 0.6\n",
    "# # --- Receivers: ----\n",
    "# head_orient = np.array([0, 0]) \n",
    "# head_pos= np.array([room[0]/2, room[1]/2, 1.3]) # Listener coordinates\n",
    "# ears_pos=hlp.head_2_ku_ears(head_pos,head_orient)\n",
    "# mics = np.array([ears_pos[0], ears_pos[1], list(head_pos)]) #two positions for the binaural, one for the SH\n",
    "# # --- Sources: ----\n",
    "# TARGET1_ANGLE=30\n",
    "# TARGET2_ANGLE=-30\n",
    "# src1_pos= hlp.place_on_circle(head_pos,1,TARGET1_ANGLE)\n",
    "# src2_pos= hlp.place_on_circle(head_pos,1,TARGET2_ANGLE)\n",
    "# srcs=np.array(src1_pos+src2_pos)\n",
    "# # --- Audio: ----\n",
    "# src1_sig_mono, fs_src1 = sf.read(pjoin(speech_path, 'dial1p1.wav'))\n",
    "# src2_sig_mono, fs_src2 = sf.read(pjoin(speech_path, 'dial1p2.wav'))\n",
    "# noise_sig_sh, fs_noise = sf.read(pjoin(arte_path,'02_Office_MOA_31ch/02_Office_MOA_31ch.wav'))\n",
    "# chunk_len = 25 #seconds\n",
    "\n",
    "\n",
    "# -------- Room parameter adjustments - same for all the scenes: ----------\n",
    "# Compute absorption coefficients for desired rt60 and room dimensions\n",
    "abs_walls,rt60_true = srs.find_abs_coeffs_from_rt(room, rt60)\n",
    "# Small correction for sound absorption coefficients:\n",
    "if sum(rt60_true-rt60>0.05*rt60_true)>0 :\n",
    "    abs_walls,rt60_true = srs.find_abs_coeffs_from_rt(room, rt60_true + abs(rt60-rt60_true))\n",
    "# Generally, we simulate up to RT60:\n",
    "limits = np.minimum(rt60, maxlim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{room.shape=} -> Room dimensions in cartesian coordinates. Dimension = (3) [x, y, z].\")\n",
    "print(f\"{abs_walls.shape=} -> Wall absorption coefficients per band. Dimension = (nBands, 6).\")\n",
    "print(f\"{limits.shape=} -> Maximum echogram computation time per band.  Dimension = (nBands).\")\n",
    "print(f\"{srcs.shape=} -> Source position in cartesian coordinates. Dimension = (nSrc, 3) [[x, y, z]].\")\n",
    "print(f\"{mics.shape=} -> Receiver position in cartesian coordinates. Dimension = (nRec, 3) [[x, y, z]].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- COMPUTE ECHOGRAMS ----------------\n",
    "# How many reflections, at what time and from which coordinates:\n",
    "abs_echograms = srs.compute_echograms_sh(room, srcs, mics, abs_walls, limits, ambi_order)\n",
    "\n",
    "# --------------- RENDER ECHOGRAMS ----------------\n",
    "# based on echograms, RIRs in spherical harmonics are generated for each defined receiver\n",
    "# here there are 3 receivers: head center, left ear, right ear\n",
    "band_centerfreqs=np.array([1000])\n",
    "rirs_sh = srs.render_rirs_sh(abs_echograms, band_centerfreqs, fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check array dimensions:\n",
    "print(f\"{abs_echograms.shape=} -> Rendered echograms. Dimension = (nSrc, nRec, nBands)\")\n",
    "print(f\"{rirs_sh.shape=} -> Rendered RIR in SH. Dimension = (M, maxSH, nRec, nSrc)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hlp.plot_scene_raw(room,mics,srcs,perspective=\"xy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------- SIGNALS IN SPHERICAL HARMONICS -----------------\n",
    "# Signal in mono is convolved with RIR in SH generated previously \n",
    "\n",
    "# make sure both loaded mono files have the same level \n",
    "src1_sig_mono=hlp.set_level(src1_sig_mono,-30)\n",
    "src2_sig_mono=hlp.set_level(src2_sig_mono,-30)\n",
    "\n",
    "# resample all audio signals:\n",
    "src1_sig_mono = sig.resample_poly(src1_sig_mono, fs, fs_src1)\n",
    "src2_sig_mono = sig.resample_poly(src2_sig_mono, fs, fs_src2)\n",
    "noise_sig_sh = sig.resample_poly(noise_sig_sh, fs, fs_noise)\n",
    "# crop all audio signals:\n",
    "src1_sig_mono = src1_sig_mono[:chunk_len*fs] \n",
    "src2_sig_mono = src2_sig_mono[:chunk_len*fs]\n",
    "L_after_convol=(chunk_len*fs)+rirs_sh.shape[0]-1\n",
    "noise_sig_sh = noise_sig_sh[:L_after_convol] # to make them all equal in SH domain\n",
    "\n",
    "\n",
    "# ---> Note! Functions mono2sh and mono2biSH expect a 2-dim array with a signal and a\n",
    "# 4-dim array with RIRs, therefore I use the python slicing - to pick the RIRs for \n",
    "# the third receiver (here corresponding with the center of the head), I use \n",
    "# rirs_sh[:,:,2:3,:] - this picks the correct elements without changing the size of\n",
    "# the array. I also change shape of the signal from (D,) to (1,D).\n",
    "\n",
    "# change shape from (D,) to (1,D) bc this is expected by :\n",
    "src1_sig_mono=np.array(src1_sig_mono,ndmin=2)\n",
    "src2_sig_mono=np.array(src2_sig_mono,ndmin=2)\n",
    "\n",
    "# from mono to spherical harmonics (sh for the head center)\n",
    "src1_sig_sh = mono2sh(src1_sig_mono, rirs_sh[:, :, 2:3, 0:1]) # rirs_sh -> (M, maxSH, nRec, nSrc)\n",
    "src2_sig_sh = mono2sh(src2_sig_mono, rirs_sh[:, :, 2:3, 1:2]) # rirs_sh -> (M, maxSH, nRec, nSrc)\n",
    "\n",
    "# from mono to binaural spherical harmonics (sh for each ear)\n",
    "src1_sig_sh_bi = mono2biSH(src1_sig_mono, rirs_sh[:, :, 0:2, 0:1])\n",
    "src2_sig_sh_bi = mono2biSH(src2_sig_mono, rirs_sh[:, :, 0:2, 1:2])\n",
    "\n",
    "# source signals are generated in 10th order ambisonics (=121 channels) \n",
    "# noise files have only 31 channels, so to sum them we pad noise signal \n",
    "# with zeros to 121 channels \n",
    "zeropads=np.zeros([noise_sig_sh.shape[0],121-31])\n",
    "noise_sig_sh =np.concatenate((noise_sig_sh,zeropads),axis=1)\n",
    "\n",
    "\n",
    "# create a binaural version of the noise for the purpose of decoding later\n",
    "noise_sig_sh_bi=np.tile(noise_sig_sh,(2,1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check array dimensions:\n",
    "print(f\"{src1_sig_sh.shape=}\")\n",
    "print(f\"{src2_sig_sh.shape=}\")\n",
    "print(f\"{src1_sig_sh_bi.shape=}\")\n",
    "print(f\"{src2_sig_sh_bi.shape=}\")\n",
    "print(f\"{noise_sig_sh.shape=}\")\n",
    "print(f\"{noise_sig_sh_bi.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- LOAD AVAILABLE DECODERS ----------------\n",
    "\n",
    "# load bimagls decoders created in matlab\n",
    "# bimagls takes RIRs in SH of 2 receivers \n",
    "decoder_ku_bimag = mat73.loadmat(pjoin('decoders_ord10', 'Ku100_ALFE_Window_sinEQ_bimag.mat'))['hnm']\n",
    "decoder_ric_bimag = mat73.loadmat(pjoin('decoders_ord10', 'RIC_Front_Omni_ALFE_Window_SinEQ_bimag.mat'))['hnm']\n",
    "\n",
    "# create a magls decoder with spaudiopy using a sofa file\n",
    "# magls takes RIRs in SH of 1 receivers \n",
    "hrirs = spa.io.load_sofa_hrirs('sofas/RIC_Front_Omni_48000Hz.sofa')\n",
    "left, rigth, new_fs = spa.process.resample_hrirs(hrirs.left, hrirs.right, hrirs.fs, fs, 8)\n",
    "hrirs.fs = new_fs\n",
    "hrirs.update_hrirs(left, rigth)\n",
    "decoder_oldb_mag = spa.decoder.magls_bin(hrirs, 10)\n",
    "decoder_oldb_mag=decoder_oldb_mag.T\n",
    "\n",
    "print(f\"{decoder_ku_bimag.shape=}\")\n",
    "print(f\"{decoder_ric_bimag.shape=}\")\n",
    "print(f\"{decoder_oldb_mag.shape=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- SET SNR AND DECODE FROM SH TO BINAURAL ----------------\n",
    "\n",
    "# Choose 1 from the loaded decoders:\n",
    "decoder=decoder_ku_bimag\n",
    "\n",
    "# Important -> depending on the decoder there is a different input signal and decoding function!!! \n",
    "dec_type=\"ku100bimagls\"\n",
    "\n",
    "\n",
    "if \"magls\" in dec_type:\n",
    "    # add speech sources in the SH domain:\n",
    "    added_sources_sh=src1_sig_sh + src2_sig_sh\n",
    "    # decode to binaural\n",
    "    added_sources_bin=sh2bin(added_sources_sh, decoder)\n",
    "    # compute the initial snr between added sources and noise\n",
    "    ini_snr = 10 * np.log10(hlp.power(added_sources_bin) / hlp.power(sh2bin(noise_sig_sh, decoder)))\n",
    "    # scale noise to achieve a desired snr: \n",
    "    noise_gain_db = ini_snr - target_snr\n",
    "    noise_sig_sh_scaled = noise_sig_sh * np.power(10, noise_gain_db/20)\n",
    "    # check if snr is correct\n",
    "    snr_check = 10 * np.log10(hlp.power(added_sources_bin) / hlp.power(sh2bin(noise_sig_sh_scaled, decoder)))\n",
    "    print(f\"{snr_check=}\")\n",
    "    # add all signals in the SH domain:\n",
    "    mixture_sh=src1_sig_sh + src2_sig_sh + noise_sig_sh_scaled\n",
    "    # decode to binaural\n",
    "    mixture_bin=sh2bin(mixture_sh, decoder)\n",
    "    # normalize SH mixture so that the binaural mixture is in [-1, 1] and it doesn't clip\n",
    "    norm_fact = np.max(np.abs(mixture_bin))\n",
    "    mixture_sh_norm = mixture_sh/norm_fact\n",
    "    mixture_bin_norm=sh2bin(mixture_sh_norm, decoder)\n",
    "elif \"bimagls\" in dec_type:\n",
    "    # add speech sources in the SH domain:\n",
    "    added_sources_sh=src1_sig_sh_bi + src2_sig_sh_bi\n",
    "    # decode to binaural\n",
    "    added_sources_bin=biSH2bin(added_sources_sh, decoder)\n",
    "    # compute the initial snr between added sources and noise\n",
    "    ini_snr = 10 * np.log10(hlp.power(added_sources_bin) / hlp.power(biSH2bin(noise_sig_sh_bi, decoder)))\n",
    "    # scale noise to achieve a desired snr: \n",
    "    noise_gain_db = ini_snr - target_snr\n",
    "    noise_sig_sh_bi_scaled = noise_sig_sh_bi * np.power(10, noise_gain_db/20)\n",
    "    # check if snr is correct\n",
    "    snr_check = 10 * np.log10(hlp.power(added_sources_bin) / hlp.power(biSH2bin(noise_sig_sh_bi_scaled, decoder)))\n",
    "    print(f\"{snr_check=}\")\n",
    "    # add all signals in the SH domain:\n",
    "    mixture_sh_bi=src1_sig_sh_bi + src2_sig_sh_bi + noise_sig_sh_bi_scaled\n",
    "    # decode to binaural\n",
    "    mixture_bin=biSH2bin(mixture_sh_bi, decoder)\n",
    "    # normalize SH mixture so that the binaural mixture is in [-1, 1] and it doesn't clip\n",
    "    norm_fact = np.max(np.abs(mixture_bin))\n",
    "    mixture_sh_bi_norm = mixture_sh_bi/norm_fact\n",
    "    mixture_bin_norm=biSH2bin(mixture_sh_bi_norm, decoder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(added_sources_bin, rate=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(mixture_bin, rate=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.write(pjoin(output_path,f\"dialog_{scene_tag}_bin_snr{int(target_snr)}_mixture_dec_{dec_type}_{fs}hz.wav\"), mixture_bin.T, fs, subtype='FLOAT')\n",
    "sf.write(pjoin(output_path,f\"dialog_{scene_tag}_bin_snr{int(target_snr)}_speech_dec_{dec_type}_{fs}hz.wav\"), added_sources_bin.T, fs, subtype='FLOAT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LibriMix3D",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
